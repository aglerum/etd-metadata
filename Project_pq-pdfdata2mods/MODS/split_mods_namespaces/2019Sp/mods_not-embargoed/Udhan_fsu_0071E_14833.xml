<?xml version="1.0" encoding="UTF-8"?>
<mods:mods xmlns="http://www.loc.gov/mods/v3"
           xmlns:flvc="info:flvc/manifest/v1"
           xmlns:dcterms="http://purl.org/dc/terms/"
           xmlns:etd="http://www.ndltd.org/standards/metadata/etdms/1.0/"
           xmlns:mods="http://www.loc.gov/mods/v3"
           xmlns:xlink="http://www.w3.org/1999/xlink"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd"
           version="3.6">
   <mods:titleInfo lang="eng">
      <mods:title>Speaker-Dependent Acoustic Emotion Recognition for Vehicle-Centric Applications</mods:title>
   </mods:titleInfo>
   <mods:name type="personal" authority="local">
      <mods:namePart type="family">Udhan</mods:namePart>
      <mods:namePart type="given">Tejal</mods:namePart>
      <mods:affiliation>Theses and Dissertations</mods:affiliation>
      <mods:role>
         <mods:roleTerm authority="local" type="text">author</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="personal"
              authority="etd-naf"
              authorityURI="https://authorities.lib.fsu.edu/etd-naf/"
              valueURI="https://authorities.lib.fsu.edu/etd-naf/swalker">
      <mods:namePart type="family">Bernadin</mods:namePart>
      <mods:namePart type="given">Shonda</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">Professor Directing Dissertation</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="personal"
              authority="naf"
              authorityURI="http://id.loc.gov/authorities/names/"
              valueURI="http://id.loc.gov/authorities/names/no2006090378">
      <mods:namePart type="family">Sobanjo</mods:namePart>
      <mods:namePart type="given">John Olusegun</mods:namePart>
      <mods:namePart type="termsOfAddress">1958-</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">University Representative</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="personal"
              authority="etd-naf"
              authorityURI="https://authorities.lib.fsu.edu/etd-naf/"
              valueURI="https://authorities.lib.fsu.edu/etd-naf/foo">
      <mods:namePart type="family">Foo</mods:namePart>
      <mods:namePart type="given">Simon Y.</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">Committee Member</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="personal"
              authority="naf"
              authorityURI="http://id.loc.gov/authorities/names/"
              valueURI="http://id.loc.gov/authorities/names/no2009155075">
      <mods:namePart type="family">Harvey</mods:namePart>
      <mods:namePart type="given">Bruce A.</mods:namePart>
      <mods:namePart type="termsOfAddress">1961-</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">Committee Member</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="corporate"
              authority="naf"
              authorityURI="http://id.loc.gov/authorities/names/"
              valueURI="http://id.loc.gov/authorities/names/n80126238">
      <mods:namePart>Florida State University</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="rda" type="text">degree granting institution</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="corporate" authority="local">
      <mods:namePart>FAMU-FSU College of Engineering</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">degree granting college</mods:roleTerm>
      </mods:role>
   </mods:name>
   <mods:name type="corporate" authority="local">
      <mods:namePart>Department of Electrical and Computer Engineering</mods:namePart>
      <mods:role>
         <mods:roleTerm authority="local" type="text">degree granting department</mods:roleTerm>
         <roleTerm authority="marcrelator" type="code">dgg</roleTerm>
      </mods:role>
   </mods:name>
   <mods:typeOfResource>text</mods:typeOfResource>
   <mods:genre authority="rdacontent">text</mods:genre>
   <mods:genre authority="coar"
               authorityURI="http://purl.org/coar/resource_type"
               valueURI="http://purl.org/coar/resource_type/c_db06">doctoral thesis</mods:genre>
   <mods:originInfo>
      <mods:place>
         <mods:placeTerm type="text">Tallahassee, Florida</mods:placeTerm>
      </mods:place>
      <mods:dateIssued encoding="w3cdtf" keyDate="yes">2018</mods:dateIssued>
      <mods:publisher>Florida State University</mods:publisher>
      <issuance>monographic</issuance>
   </mods:originInfo>
   <mods:language>
      <mods:languageTerm type="text">English</mods:languageTerm>
      <mods:languageTerm type="code" authority="iso639-2b">eng</mods:languageTerm>
   </mods:language>
   <mods:physicalDescription>
      <mods:form authority="rdamedia" type="RDA media terms">computer</mods:form>
      <mods:form authority="rdacarrier" type="RDA carrier terms">online resource</mods:form>
      <mods:extent>1 online resource (94 pages)</mods:extent>
      <mods:digitalOrigin>born digital</mods:digitalOrigin>
      <mods:internetMediaType>application/pdf</mods:internetMediaType>
   </mods:physicalDescription>
   <mods:abstract>Speech is the most natural and fastest method of communication between humans. This fact compelled researchers to study acoustic signals as a fast and efficient means of interaction between humans and machines. For authentic human-machine interaction, the method requires that the machines should have the sufficient intelligence to recognize human voices and their emotional state. It is well-known that the emotional state of human drivers highly influences his/her driving performance. For example, there are many reports that describe road-rage incidents where drivers become emotionally enraged due to the actions of another driver. This anger may lead to a high-speed chase, tailgating, and sometimes even death due to a traffic crash or physical contact. If a car is ‘intelligent-enough’ to respond to a driver’s emotional state, it may be able to thwart negative outcomes of road-rage incidents. Speech emotion recognition, extracting the emotional state of speakers from acoustic data, plays an important role in enabling machines to be ‘intelligent’. Speech emotion recognition is an emerging field and presents many challenges. The set of most powerful features which can distinguish different emotions is not defined; hence, the selection of features is a critical task. Acoustic variability presented by numerous speech properties, such as length and complexity of human speech utterance, speaker’s gender, speaking styles and rate of speech, directly affects the most common speech features; thereby affecting the system performance. Most of the researchers used statistical approaches to recognize human speech; however statistical methods are complex and need more computational time. Moreover, emotion recognition being the developing field, researchers are exploring facial, gestural and acoustical features for emotion recognition. However, for vehicle-centric applications, audio and speech processing may provide better noninvasive and less distracting solutions than other interactive in-vehicle infotainment systems. Hence, acoustic feature extraction for emotion recognition in human drivers is a preferred design choice of this research. The goal of this research is to develop an optimal feature extraction algorithm for emotion recognition of four most common emotions (anger, happy, sad and no emotion). In this dissertation, six acoustic features are studied using decision-tree based algorithms to recognize speech-based human emotions and reduce the complexity of the system. The speech features used are pitch, intensity, frequency formants, jitter, shimmer and zero crossing rate. Pitch and intensity are qualitative voice feature, frequency formants and jitter provide the spectral features and zero-crossing rate and shimmer suffice as temporal features of human acoustical speech. The combination of different types of speech features is utilized to increase the accuracy of system. The decision tree-based algorithms are designed in MATLAB and are calculated using confidentiality-interval for each feature. For acoustic data visualization, PRAAT software is used. The system is designed for speaker-dependent emotion recognition since the accuracy of system is more as the utilized features are qualitative voice features; which are best-suited for emotion recognition. Data from two males and two females is analyzed for this dissertation. For the actual realization of system, noise analysis is performed using 5dB, and 15 dB signal-to-noise ratio levels. These are minimum and maximum noise levels experienced while driving on a freeway and parking lot. This dissertation is composed of five chapters. Chapter 1 presents the mechanism of human speech production and human emotions in speech. It comprises of various emotions and importance of acoustic signal for emotion recognition. Chapter 2 includes different local and global acoustic features, existing methods of speech recognition and emotion recognition and discusses the weaknesses of existing speech recognition systems for acoustical emotion recognition using various acoustic features and analysis algorithms. Chapter 3 outlines the proposed solution for acoustic emotion recognition using decision-tree based algorithm. It includes a description of each acoustical feature, data preparation techniques, data analysis methods, and algorithm design. Chapter 4 consists of results, discussion and comparison of proposed algorithm with state-of-the-art acoustic emotion recognition algorithms. Finally, conclusion, limitations and future work is discussed in chapter 5.</mods:abstract>
   <mods:note displayLabel="Submitted Note">A Dissertation submitted to the Department of Electrical and Computer Engineering in partial fulfillment of the requirements for the degree of Doctor of Philosophy.</mods:note>
   <mods:note displayLabel="Degree Awarded">Fall Semester 2018.</mods:note>
   <mods:note displayLabel="Date of Defense">November 29, 2018.</mods:note>
   <mods:note displayLabel="Keywords">Acoustic Emotion Recognition, Acoustic Features, Speech Processing</mods:note>
   <mods:note displayLabel="Bibliography Note">Includes bibliographical references.</mods:note>
   <mods:note displayLabel="Advisory Committee">Shonda Bernadin, Professor Directing Dissertation; John O. Sobanjo, University Representative; Simon Foo, Committee Member; Bruce Harvey, Committee Member.</mods:note>
   <mods:subject authority="lcsh"
                 authorityURI="http://id.loc.gov/authorities/subjects/"
                 valueURI="http://id.loc.gov/authorities/subjects/sh85041666">
      <mods:topic>Electrical engineering</mods:topic>
   </mods:subject>
   <identifier type="IID">2019_Spring_Udhan_fsu_0071E_14833</identifier>
   <mods:extension>
      <etd:degree>
         <etd:name>Doctor of Philosophy</etd:name>
         <etd:level>Doctoral</etd:level>
         <etd:discipline>Electrical and Computer Engineering</etd:discipline>
      </etd:degree>
      <flvc:flvc>
         <flvc:owningInstitution>FSU</flvc:owningInstitution>
         <flvc:submittingInstitution>FSU</flvc:submittingInstitution>
      </flvc:flvc>
   </mods:extension>
   <mods:recordInfo>
      <mods:recordCreationDate encoding="w3cdtf">2019-10-14T16:36:31.241-04:00</mods:recordCreationDate>
      <descriptionStandard>rda</descriptionStandard>
   </mods:recordInfo>
</mods:mods>
